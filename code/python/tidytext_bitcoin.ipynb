{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(tidytext)\n",
    "library(stringr)\n",
    "library(tidyverse)\n",
    "library(tibble)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(SnowballC)\n",
    "library(RSQLite)\n",
    "library(sparklyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc <- spark_connect(master = \"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Tweet ID, Conversation ID, Author Id , Author Name, isVerified, DateTime, Language, Tweet Text, Replies, Retweets, Favorites, Mentions, Hashtags, Permalink, URLs, isPartOfConversation, isReply, isRetweet, Reply To User ID, Reply To User Name, Quoted Tweet ID, Quoted Tweet User Name, Quoted Tweet User ID'"
      ],
      "text/latex": [
       "'Tweet ID, Conversation ID, Author Id , Author Name, isVerified, DateTime, Language, Tweet Text, Replies, Retweets, Favorites, Mentions, Hashtags, Permalink, URLs, isPartOfConversation, isReply, isRetweet, Reply To User ID, Reply To User Name, Quoted Tweet ID, Quoted Tweet User Name, Quoted Tweet User ID'"
      ],
      "text/markdown": [
       "'Tweet ID, Conversation ID, Author Id , Author Name, isVerified, DateTime, Language, Tweet Text, Replies, Retweets, Favorites, Mentions, Hashtags, Permalink, URLs, isPartOfConversation, isReply, isRetweet, Reply To User ID, Reply To User Name, Quoted Tweet ID, Quoted Tweet User Name, Quoted Tweet User ID'"
      ],
      "text/plain": [
       "[1] \"Tweet ID, Conversation ID, Author Id , Author Name, isVerified, DateTime, Language, Tweet Text, Replies, Retweets, Favorites, Mentions, Hashtags, Permalink, URLs, isPartOfConversation, isReply, isRetweet, Reply To User ID, Reply To User Name, Quoted Tweet ID, Quoted Tweet User Name, Quoted Tweet User ID\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system('head -n 1 ../../data/bitcoinoct17tooct18/all_sorted.csv', intern = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'1049449307043024896,1049449307043024896,768679436,tapatrade,0,2018-10-08 19:59,und,#bitcoin https://twitter.com/MustStopMurad/status/1022169639386836992 …,0,0,0,,#bitcoin,/tapatrade/status/1049449307043024896,https://twitter.com/MustStopMurad/status/1022169639386836992,0,0,1,,,1022169639386836992,MustStopMurad,844304603336232960'"
      ],
      "text/latex": [
       "'1049449307043024896,1049449307043024896,768679436,tapatrade,0,2018-10-08 19:59,und,\\#bitcoin https://twitter.com/MustStopMurad/status/1022169639386836992 …,0,0,0,,\\#bitcoin,/tapatrade/status/1049449307043024896,https://twitter.com/MustStopMurad/status/1022169639386836992,0,0,1,,,1022169639386836992,MustStopMurad,844304603336232960'"
      ],
      "text/markdown": [
       "'1049449307043024896,1049449307043024896,768679436,tapatrade,0,2018-10-08 19:59,und,#bitcoin https://twitter.com/MustStopMurad/status/1022169639386836992 …,0,0,0,,#bitcoin,/tapatrade/status/1049449307043024896,https://twitter.com/MustStopMurad/status/1022169639386836992,0,0,1,,,1022169639386836992,MustStopMurad,844304603336232960'"
      ],
      "text/plain": [
       "[1] \"1049449307043024896,1049449307043024896,768679436,tapatrade,0,2018-10-08 19:59,und,#bitcoin https://twitter.com/MustStopMurad/status/1022169639386836992 …,0,0,0,,#bitcoin,/tapatrade/status/1049449307043024896,https://twitter.com/MustStopMurad/status/1022169639386836992,0,0,1,,,1022169639386836992,MustStopMurad,844304603336232960\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system('tail -n 1 ../../data/bitcoinoct17tooct18/all_sorted.csv', intern = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Source:   lazy query [?? x 23]\n",
       "# Database: spark_connection\n",
       "  Tweet_ID Conversation_ID Author_Id Author_Name isVerified DateTime Language\n",
       "     <dbl>           <dbl>     <dbl> <chr>            <int> <chr>    <chr>   \n",
       "1  9.62e17         9.62e17   9.04e17 CryptoCoin…          0 2018-02… bg      \n",
       "2  9.62e17         9.62e17   9.04e17 CryptoCoin…          0 2018-02… bg      \n",
       "3  9.62e17         9.62e17   8.82e17 NowBitcoin…          0 2018-02… ca      \n",
       "4  9.62e17         9.62e17   8.94e17 falileevam4          0 2018-02… de      \n",
       "5  9.62e17         9.62e17   9.55e17 cryptoaudio          0 2018-02… en      \n",
       "6  9.62e17         9.62e17   9.59e17 ADATicker            0 2018-02… en      \n",
       "# ... with 16 more variables: Tweet_Text <chr>, Replies <int>, Retweets <int>,\n",
       "#   Favorites <int>, Mentions <chr>, Hashtags <chr>, Permalink <chr>,\n",
       "#   URLs <chr>, isPartOfConversation <chr>, isReply <chr>, isRetweet <chr>,\n",
       "#   Reply_To_User_ID <chr>, Reply_To_User_Name <chr>, Quoted_Tweet_ID <chr>,\n",
       "#   Quoted_Tweet_User_Name <chr>, Quoted_Tweet_User_ID <chr>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#con = dbConnect(SQLite(), dbname=\"../../data/twitter.db\")\n",
    "tweets_df = dbGetQuery(con, 'select * from bitcoin where DateTime > \"2018-03-03\" and DateTime < \"2018-05-03\" and Language=\"en\"')\n",
    "#tweets_df <- spark_read_csv(sc, name = \"test\", '/home/apcloud44/latent_aspect_modeling/data/bitcoinoct17tooct18/all_sorted.csv')\n",
    "head(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweets_df <- read.csv('../../data/bitcoinoct17tooct18/all_sorted.csv', nrow=261079)\n",
    "#tweets_df <- spark_read_csv(sc = sc, name='bitcoint_df', \n",
    "#                            path = spark_normalize_path('../../data/bitcoinoct17tooct18/all_sorted.csv'), \n",
    "#                                                        header = TRUE, delimiter = \",\") \n",
    "\n",
    "#%>% filter(DateTime > \"2018-03-03\" & DateTime < \"2018-05-03\" & Language == \"en\")\n",
    "#head(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 39, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:248)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:50)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1347)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:48)\n\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2727)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2727)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2727)\n\tat sparklyr.Utils$.collect(utils.scala:196)\n\tat sparklyr.Utils.collect(utils.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat sparklyr.Invoke.invoke(invoke.scala:137)\n\tat sparklyr.StreamHandler.handleMethodCall(stream.scala:123)\n\tat sparklyr.StreamHandler.read(stream.scala:66)\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:51)\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:4)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOut\n",
     "execution_count": 24,
     "output_type": "error",
     "traceback": [
      "Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 39, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)\n\tat java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:248)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:50)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:48)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1347)\n\tat org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:48)\n\tat java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2727)\n\tat org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2727)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2727)\n\tat sparklyr.Utils$.collect(utils.scala:196)\n\tat sparklyr.Utils.collect(utils.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat sparklyr.Invoke.invoke(invoke.scala:137)\n\tat sparklyr.StreamHandler.handleMethodCall(stream.scala:123)\n\tat sparklyr.StreamHandler.read(stream.scala:66)\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:51)\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:4)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOut\nTraceback:\n",
      "1. tweets_df %>% collect %>% filter(!str_detect(Tweet_Text, \"^RT\")) %>% \n .     mutate(text = stringr::str_remove_all(Tweet_Text, remove_reg)) %>% \n .     unnest_tokens(word, text, token = \"tweets\") %>% mutate(word = wordStem(word, \n .     language = \"english\")) %>% filter(!word %in% stop_words$word, \n .     !word %in% str_remove_all(stop_words$word, \"'\"), str_detect(word, \n .         \"[a-z]\"))",
      "2. withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))",
      "3. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "4. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "5. `_fseq`(`_lhs`)",
      "6. freduce(value, `_function_list`)",
      "7. function_list[[i]](value)",
      "8. collect(.)",
      "9. collect.tbl_sql(.)",
      "10. db_collect(x$src$con, sql, n = n, warn_incomplete = warn_incomplete)",
      "11. db_collect.DBIConnection(x$src$con, sql, n = n, warn_incomplete = warn_incomplete)",
      "12. tryCatch({\n  .     out <- dbFetch(res, n = n)\n  .     if (warn_incomplete) {\n  .         res_warn_incomplete(res, \"n = Inf\")\n  .     }\n  . }, finally = {\n  .     dbClearResult(res)\n  . })",
      "13. tryCatchList(expr, classes, parentenv, handlers)",
      "14. dbFetch(res, n = n)",
      "15. dbFetch(res, n = n)",
      "16. .local(res, n, ...)",
      "17. df_from_sdf(res@conn, res@sdf, end)",
      "18. sdf_collect(sdf)",
      "19. invoke_static(sc, \"sparklyr.Utils\", \"collect\", sdf, separator$regexp)",
      "20. invoke_static.spark_shell_connection(sc, \"sparklyr.Utils\", \"collect\", \n  .     sdf, separator$regexp)",
      "21. invoke_method(sc, TRUE, class, method, ...)",
      "22. invoke_method.spark_shell_connection(sc, TRUE, class, method, \n  .     ...)",
      "23. core_invoke_method(sc, static, object, method, ...)",
      "24. withr::with_options(list(warning.length = 8000), {\n  .     if (nzchar(msg)) {\n  .         core_handle_known_errors(sc, msg)\n  .         stop(msg, call. = FALSE)\n  .     }\n  .     else {\n  .         msg <- core_read_spark_log_error(sc)\n  .         stop(msg, call. = FALSE)\n  .     }\n  . })",
      "25. force(code)",
      "26. stop(msg, call. = FALSE)"
     ]
    }
   ],
   "source": [
    "remove_reg <- \"&amp;|&lt;|&gt;\"\n",
    "unnest_reg  <- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n",
    "\n",
    "tidy_tweets <- tweets_df %>% \n",
    "\n",
    "\n",
    "#collect %>%\n",
    "\n",
    "filter(!str_detect(Tweet_Text, \"^RT\")) %>%\n",
    "mutate(text = stringr::str_remove_all(Tweet_Text, remove_reg)) %>%\n",
    "\n",
    "\n",
    "\n",
    "unnest_tokens(word, text, token = \"tweets\") %>%\n",
    "mutate(word=wordStem(word, language = 'english')) %>%\n",
    "\n",
    "filter(!word %in% stop_words$word,\n",
    "     !word %in% str_remove_all(stop_words$word, \"'\"),\n",
    "     str_detect(word, \"[a-z]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets$DateTime <- tidy_tweets$DateTime %>% as.Date(format = '%Y-%m-%d')\n",
    "str(tidy_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(tidy_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets %>% count(DateTime) %>% \n",
    "ggplot() + aes(x=DateTime, y=n) + geom_line() +geom_point() + ylab('No of Tweets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate frequency of words against each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets_orig <- tidy_tweets %>% mutate(week = week(DateTime)) %>% group_by(week) %>% count(word, sort=TRUE) %>% \n",
    "left_join(tidy_tweets %>% mutate(week = week(DateTime)) %>% group_by(week) %>% summarize(total=n())) %>% \n",
    "mutate(freq = n/total) %>% mutate(logn = round(log(n))) %>% select(week, word, freq, n, logn, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=500\n",
    "tidy_tweets_freq <- tidy_tweets_orig %>% arrange(desc(n))  %>% group_by(week) %>% top_n(freq, n=n) \n",
    "tidy_tweets_freq %>% arrange(week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert words to columns (dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets_plot <- tidy_tweets_freq %>% select(-n,-freq, -logn) %>% spread(week, freq, fill = 0)\n",
    "tidy_tweets_ca <- tidy_tweets_freq %>% select(-n, -freq,-total) %>% spread(week, logn, fill = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRDS(tidy_tweets_ca, file = paste0('../../data/tidy_tweets_per_week_log_bitcoin_',n,'.rds'))\n",
    "tidy_tweets_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRDS(tidy_tweets_plot, file = paste0('../../data/tidy_tweets_per_week_n_bitcoin_',n,'.rds'))\n",
    "tidy_tweets_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy_tweets_plot %>% ggplot() + aes('9','10') + geom_jitter() + geom_text(aes(label = word), check_overlap = TRUE, vjust = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
